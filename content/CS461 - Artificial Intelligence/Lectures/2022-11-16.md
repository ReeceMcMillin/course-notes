---
title: "2022-11-16"
date: 2022-11-16
tags:
- lectures
- cs461
---

Started with a discussion on [Greg Rutkowski's unwanted influence in AI image generation.]([https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/](https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/)[](https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/))

* [Shoshana Zuboff - The Age of Surveillance Capitalism](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism)

Three questions
* **Knowledge**: *what* is known and *who* knows it?
* **Legitimacy**: who says it's *okay* for them to know it?
* **Power**: who controls that legitimacy?

Most often we use our network for classification. One neuron per output. 1-hot classification if we want a solid answer, softmax if we want a probability distribution.

This isn't bad, but every network has a chance of error. What if we train a network to identify misclassifications by another network? This is what's called an **ensemble** method.
* First practical applications were mail-sorting by reading handwritten digits.
* Instead of *a* neural network, train 3 networks.
	* Train the *first* network on the full data set, then test on the full data set.
	* Train the *second* network on the cases that the first network *missed*, test on the full data set.
	* Train the *third* network on cases that the first and second networks differed (tie-breaker).
* If you can get 2 out of 3 networks to agree, you've got yourself a classification!
* Assume your training is good. Increase weights of things that were *misclassified*, decrease correct classifications.

![[CS461 - Artificial Intelligence/Lectures/Untitled Diagram.svg]]


