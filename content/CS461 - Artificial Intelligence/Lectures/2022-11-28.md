---
title: "2022-11-28"
date: 2022-11-28
tags:
- lecture
- cs461
---

* [Cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy)
* What should the learning rate be?
	* No hard-and-fast rule, gonna take some trial and error.
	* Higher learning rates generally converge faster, but with less accuracy.
		* If you need better accuracy, slow your learning rate!
* [Rectified Linear Unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
* Batch Normalization
* Overfitting: you're memorizing your test data
* Optimal Brain Damage: remove neurons that aren't contributing
* Dropout: *randomly* select a portion of hidden neurons and don't use them *for a particular batch*.
	* for each batch, you're training a very slightly different network which helps prevent overfitting.
* Regularization/weight decay
	* L2 regularization is common
* Can sometimes modify the training set
	* Add noise, shift pixels, rotate image, etc.
* Batch Normalization
	* Fairly advanced technique - normalize each set of weighted sum coming into neurons
* Ensemble Methods
* Unstable Gradient
* Regression Networks
	* approximately $f(inputs)$ over some range of inputs
		* $f(x_1, x_2, \dots, x_i)$
	* remember from calculus that any continuous function can be approximated by a sum of Gaussians
	* find the best approximation you can using one gaussian
		* then the sum of two, then three, etc. until you aren't getting much better
		  ![[Excalidraw/gaussian-regression-network.svg]]
* [Bottleneck Networks](https://en.wikipedia.org/wiki/Autoencoder)
	* aka auto-encoders