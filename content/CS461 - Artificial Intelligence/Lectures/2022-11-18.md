---
title: "2022-11-18"
date: 2022-11-18
tags:
- lecture
- cs461
---

# Introduction

**Temporal Difference Learning**: the score at position $n$ ought to help predict the score at position $n+1$.

*Any* linear function can be represented by a single hidden layer (of some  sufficient size).
Nonlinear functions have to have multiple hidden layers

# Teaching a Network
* Initially randomized
	* Gaussian with mean 0
	* SD 1
	* Uniform in $(-1, 1)$
* Present network with training cases
* Repeat until all training cases are properly

# Backpropagation

* Compare known and expected values
* If predicted value was too high:
	* input weights leading to higher values are decreased, lower increased

# Backgammon

Backgammon is well-suited to neural networks because...
* Stalemate is impossible - every game eventually results with a win or loss
* Evaluation function can be treated as continuous and locally linear
	* In general, NNs do better with broad strategy vs. sharp tactics
* Dice force exploration of different parts of the game tree
	* Some randomness helps reduce the chance of a quirky evaluation function reinforcing "oddball" moves

## Building a Backgammon Network

Most of this is based on Tesauro's work in the 1990s. He'd already written an earlier program (Neurogammon) that leveraged extensive game strategy knowledge.

He was interested in seeing how well a neural network could learn given *no* game knowledge.

## Training the Network

Configuration:
* 40 neurons in 1 hidden layer
* 4 output neurons representing game or gammon for each color
* weights initialized uniformly in $(-0.5, 0.5)$

Network was set to play against *itself*
* Initially, it's obviously picking moves more or less at random
* After the first 100-200 games, both play and learning began improving quickly

Linear functions were learned first, more advanced/abstract strategy picked up over longer periods of time.

# Temporal Difference Learning

Evaluation at time $t$ should be a reasonably good predictor of evaluation at time $t+1$.
* Thus, we can use $t+1$'s output as the value that $t$ *should have* produced.
	* Learn with parameter $\lambda$
* Likewise, state $t-1$ should have been a (weaker) prediction of $t+1$.
	* Learn with parameter $\lambda^2$
* Moving back $k$ steps uses $\lambda^k$ until that falls below some small positive constant.
* When the game is completed, use output vector (e.g., $(1, 0, 0, 0)$) for each position in same process.

# How Did it Work?

The system played 200,000 games against itself and eventually reached the level of a strong casual player ("intermediate" rating).
* Remember, it did this with *no* strategy knowledge built-in!
* It only knew a representation of the board position.
* Neurons were found that corresponded to particular game strategies.

Once strategy knowledge was added explicitly, it ranked among the top human players in the world. Some neurons included:
* Number of blots within range 1-6 of opponent's pieces (direct shots)
* Number of blots within range 7-12 of oppenents pieces or hittable by 4/4, 5/5, or 6/6 (indirect shots)
* Possession of doubling cube

The next version...
* refined game knowledge
* added lookahead
	* forward pruning, provided good tactical analysis
* moved to 6 output neurons (to represent backgammons)

# Current State

Software can now beat *any human player*. These programs are used for practice, training, and analysis.

Programmers noted that computer programs tended to make certain moves (splitting back pair) sooner than theory said they should. That was assumed to be a quirk of the program until multiple versions showed the same quirk! A follow-up detailed rollout analysis showed that theory was lacking, the computer's play was superior.

# Quantifying Luck

This is the basis of *variance reduction* in rollout analysis.
* Over many games, dice will even out, but within one game they can be very uneven.
* A running total of net luck gives us a way of assessing the role of luck within a game.
* This can make 1 game played with variance reductiona s useful for analysis as 20-25 games played without it.

Aside: how does a program weaken its play?
* Add some Gaussian randomness to each move's evaluation, then resort the moves.

Aside 2: "this program cheats on dice rolls!"
* The effect of "good play" is to increase the number of future rolls on which something useful can be done, and decrease the number of rolls the opponent can do anything with. This might look like the computer gives themselves extra luck or fudges the dice rolls to perform better, but really it's just strategy farther in advance than we can recognize.