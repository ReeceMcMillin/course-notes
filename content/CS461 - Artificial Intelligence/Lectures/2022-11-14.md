---
title: "2022-11-14"
date: 2022-11-14
tags:
- lecture
- cs451
---

# Videos

1. [Deep Neural Networks are Easily Fooled](https://www.youtube.com/watch?v=M2IebCN9Ht4)

Consider the following videos assigned:
* [Playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
	* But What *Is* a Neural Network?
	* Gradient Descent: How Neural Networks Learn
	* What is Backpropagation Really Doing?
	* Appendix: Backpropagation Calculus
* [How Convolutional Neural Networks Work](https://www.youtube.com/watch?v=FmpDIaiMIeA)

# Lecture

Convolutional neural networks
* It's useful to image a pixel grid for an image, but the idea is "multidimensional"
* Essentially build up a filter

Loss Function
* Blocking a legitimate email is 10x as bad as letting a spam email through

Overtraining is *always* a risk - how do we reduce it?
1. Lower the learning rate.
	* Any time we find an adjustment we need to make, we multiple it by some constant $0 < \alpha < 1$.
	* During training, we can gradually reduce the learning rate.
		* Each epoch, set $\alpha \gets 0.999\alpha$
2. Add noise to training data.
	* Rather than seeing the same image every single time, it's seeing a dozen similar but distinct images.
	  ![[images/add-noise-strategy.svg]]
3. Mini-batch training
4. Remove some amount of parameters
5. "Optimal brain damage"
	* Doesn't scale up well
6. Dropout
	* Closely related to optimal brain damage
	* For each batch, some specified proportion are just dropped out at random
	* At each batch, you're training a *slightly* different network
7. More data!
8. Regularization
	* $L_1$ - tends to push smaller weights down faster (*sparse network*)
	* $L_2$ - tends to lead to weights getting uniformly small

[Tensorflow Playground](https://playground.tensorflow.org)

