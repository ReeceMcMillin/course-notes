---
title: "2022-12-05"
date: 2022-12-05
tags:
- lecture
- cs461
---

# Unsupervised Machine Learning

## Cluster Analysis

* K-means
	* cluster by minimizing average distance to cluster centroid
	* view data collection with $N$ data items per case as coordinates in $N$-dimensional space
	* place $K$ centroids randomly in this $N$-dimensional space.
		* curse of dimensionality: the more dimensions $N$, the more likely that any given point is an outlier in at least one dimension.
	* For each data itemm, find the distance to th enearest centroid and assign that item to that centroid.
	* Then go ahead and move that centroid to the center of the 
* K-means++
	* select 1 item with uniform probability as first centroid
	* for each item $P$, find distance to nearest centroid $D(P)$
	* select item for next centroid with probability proportional to $D^2$
	* Repeat steps 2 and 3 until all centroids are chosen
* Hierarchical clustering
	* Initially, every data item is in its own cluster
	* Repeatedly find nearest 2 clusters and merge them
	* Repeat until the desired number of clusters (usually 1) is reached
* C-means
	* a lot like K-means except ==fuzzy centroids==
	* centroid set membership based on distance from centroid
		* many variants for thresholding, inverse square/clip below some $\epsilon$/etc
* K-nearest neighbor
	* Classify items based on what their neighbors are
	* useful if some items have been classified and others still need to be decided
	* larger values of $K$ lead to larger groupings, fewer islands
	* boundaries can be sharp or probabilistic
	* works best if examples are relatively separated, tends to converge slowly if training set is large or distance calculation is complicated

## Linear Classifiers

Idea: given some classification, combine variables in a way that allows prediction.

* Support Vector Machines
	* Where linear classifiers try to identify straight-line boundaries between subgroups...
	* Multivariate divisions are *hyperplanes* dividing $N$-dimensional space!
	* Attempt more general classification, not necessarily linear
	* The input space might look tangled in a linear input space, but make perfect sense in some higher dimension!
		* Still, ==overtraining is always an issue==
* Principle Components Analysis
	* Reduce the dimensionality of a problem if we have a large number of variables per instance
	* Identify variables that are most highly correlated
		* combine variables into a small number of components
		* components can be mathematically decorrelated into independent factors
	* For example, analysis of adjectives used to describe political leaders tend to break into 2 factors: effective/ineffective, harsh/compassionate
* Association
	* Look for similarities based on items that are either in common or "close" in some sense
	* Group...
		* shoppers based on purchase history
		* movies by ratings given by users
	* various nominal and ordinal correlation measures and ways of measuring closeness of association
		* chi-square, spearman's rho, cramer's V, phi coefficient, lambda coefficient, uncertainty coefficient, others

## Hebbian Learning

* Neurons that fire together wire together
	* general idea behind backpropagation as well
* Neurons with similar activation levels have their connections strengthened
* *Self-organizing maps* place neurons into a 2D structure, arrange based on similarity
	* initially neighborhood is large (looking for global characteristics)
	* gradually reduced in size (finding local estimates)

## Method of Moments

Good for *latent variable models*, which assume that there are unobservable (latent) variables.

* Given a document, one of the latent variables may be "topic"
	* Can identify similarity of topics even if the actual *value* of the topic is unknown

## Supervised vs. Unsupervised

Supervised learning tends to be...
* computationally simpler
* more accurate

Unsupervised learning tends to be...
* less accurate
* require more interpretation

Unsupervised learning can identify atypical or anomalous data items, and so they're more useful for finding fraudulent transactions.

> [!NOTE] Book Recommendation: **The Cuckoo's Egg**
> 
> There was an attempt to hack into MIT, 50 cent accounting anomaly tipped them off, etc.

