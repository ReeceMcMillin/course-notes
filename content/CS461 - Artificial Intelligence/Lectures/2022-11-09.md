---
title: "2022-11-09"
date: 2022-11-09
tags:
- lecure
- cs461
---

# Recap

We were talking about decision trees and the issue of what decision to make next

$$H(p) = \Sigma{(-p\lg{p})}$$
Example:
* 58 students in class
* 37 got a B or higher. $\frac{37}{58} \approx 0.6379$
* 21 got a C or lower. $\frac{21}{58} \approx 0.3621$
* 26.452 bits of information


* Review session
	* 18/22 got B or above
	* 4/22 got a C or lower
	* *Information: 6.059 bits*
* Not review session
	* $\frac{19}{36}$ got a B or lower
	* $\frac{17}{36}$ got a B or lower
	* *Information: 17.953 bits*
* Total of 23.9842 bits
* Where'd the 2 bits go?
	* Explained by whether or not a student went to the review session


> [!NOTE] QUIZ
> 
> Hare won't ask us to calculate this, but will want us to understand what we're doing and why.

Advantages:
* simple
* straightforward
* easy to calculate
* works reasonably well

Briefly covered [[CS461 - Artificial Intelligence/10 - Decision Trees/ID3 Algorithm]]

# Neural Networks

*Inspired by* biology - not *modeling*. There are some similarities to how actual neurons work, but it's mostly a good analogy.

If a neuron activates (exceeds its electrical activation threshold), sends signal down the *axon*.

![[images/weights-and-biases.svg]]

Sigmoid can either be normal(?) or hyperbolic tangent

![[images/fully-connected-network.svg]]

At some point, we'll have an output layer. This is typically *one neuron per category*.

* Perceptron Learning
	* Find derivative of total sum as function of the weights: $\frac{dF}{dw}$
	* Take what we*got* vs what we *should have got*: $y^\prime - y = erv$
	* What we do to avoid overcorrecting: set a *learning rate* $\alpha$
	  $$\frac{dF}{dw}\cdot \alpha$$
	* Given an output and a target output, we know the sign, direction, and magnitude of the change we need to make to any given weight
	* If our data is *linearly separable*, we'll be good! That's a **big caveat** though
* Linearly Separable
	* For a visual metaphor, think of a bunch of red and blue on a graph. Can you draw a clear line between the two color categories?
	* If data is **not** linearly separable, a perceptron can't distinguish between them

* Gradient Descent
	* Need a partial derivative
	* Imagine we have a function $F(x, y, z)$.
	* The gradient of $F$, notated $\nabla{F}$, is
	  $$\nabla{F} = \frac{\delta{F}}{\delta{x}} + \frac{\delta{F}}{\delta{y}} + \frac{\delta{F}}{\delta{z}}$$
	* The gradient signifies the vector and direction of steepest descent on our error curve