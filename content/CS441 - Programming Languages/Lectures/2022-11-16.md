---
title: "2022-11-16"
date: 2022-11-16
tags:
- lecture
- cs441
---

# Concurrency

* Without threads, a program (such as a web browser) must either adopt a sequential approach or centralize the problem of delay-inducing events in a single **dispatch loop**.
	* The problem with this is that the complexity of dividing tasks and saving state hides the *actual algorithmic structure* of the program.
	* This approach makes the management of the tasks *explicit* and control flow *within* tasks becomes implicit. This is inside out!
* Solid interactivity must ensure that no single operation takes too long.

## Multiprocessor Architecture

* A **distributed system** is characterized by interactions among separate programs running on separate machines.
* A parallel but *nondistributed system* (a single program running on a single machine) can still be very large.
	* Historically, most parallel but nondistributed systems were homogenous - their processors were identical.
* Main memory - uniform vs. non-uniform

## Memory Coherence

Caches introduce a special problem for shared-memory machines: unless we do something, a core that has cached a given memory location may go an arbitrarily long time without seeing changes made by other cores.

This problem - how to keep cached copies of a memory location *consistent* - is the **coherence** problem.
* Solution is easy on a bus-based system. It's broadcast-based, so cache controllers can snoop on the memory traffic of other cores. This is used by most commercial microprocessors.
* What if there's no broadcast bus? How do we make sure things update in the order they're supposed to?

## Supercomputers

Cache complexity causes large, shared-memory computers to be difficult to build.

## Concurrent Programming

We usually use the term *thread* to refer to the active entity that the programmer thinks of as running concurrently to other threads.

* A **heavyweight** process has its own address space.
* A collection of **lightweight** processes may share an address space.

A *task* is a well-defined unit of work that must be done by some thread. Threads will often share a bag of tasks, each thread repeatedly removing and executing tasks from the shared bag.

## Communication and Synchronization

Two most crucial issues: communication and synchronization between threads. This is either based on **shared memory** or **message passing**.
* Shared memory: some or all of a program's variables are accessible to multiple threads.
* Message passing: threads have no shared state; one thread must carry out an explicit `send` operation to send data to the other.

For both types, synchronization can be implemented using either **spinning** (busy-waiting) or **blocking**.
* Spinning: thread runs in a loop in which it keeps re-evaluating some condition until that condition becomes true.
	* Examples: message queue is non-empty, shared variable has a particular value, etc.
	* Presumably waiting on some other thread to complete some operation.

## Languages and Libraries

* Implicit vs. explicit parallelism

## Creation Syntax

> [!NOTE] Further Reading
> 
 > Read pages 638-639 in the text!

## Launch at Elaboration

## Fork/Join

Co-begin, parallel loops, and launch-at-elaboration all lead to a pattern where thread executions are properly nested. The fork operation is more general; it makes thread creation an *explicit* operation.
* Fork: start a new thread
* Join: wait for that thread to finish, grab its values.

