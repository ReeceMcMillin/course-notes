---
title: "2022-11-18"
date: 2022-11-18
tags:
- lecture
- cs441
---

# Early Reply

# Thread Implementation

* At one extreme, every thread could get its own process.
* At the other, we could multiplex all of a program's threads on top of a *single* process.
* Intermediate: potentially large number of threads running on a smaller but nontrivial number of processes.

The typical thread implementation begins with [[CS441 - Programming Languages/9 - Subroutines and Control Abstraction/Coroutines|coroutines]]. Turning coroutines into threads has three main steps:
1. Hide the argument to transfer control by implementing a *scheduler* that chooses which thread will run next when the current thread yields the core.
2. Implement a pre-emption mechanism that suspends the current thread automatically on a regular basis so every thread gets a chance to run.
3. Allow the data structures describing our collection of threads to be shared by more than one OS process, possibly on separate cores so threads can run on any of the processes.

> [!NOTE] Further Reading
> 
> The textbook has a section on *uniprocessor scheduling* - worth a read.

## Preemption

We want to multiplex each core...
* *fairly*
* at a relatively *fine grain*
* without requiring threads to explicitly call `yield` at frequent intervals (or at all)

Many languages use **timer signals** for preemptive multithreading.
* When switching threads we ask the OS to deliver a signal to the thread at a specified time in the future.
* OS delivers the signal by saving the context (program counter, registers) of the process and transferring control to a previously specified handler in the language runtime.
* When called, the handler modifies the state of the currently running thread to make it appear the thread had just executed a call to `yield` and was about to execute its prologue.
* The handler then "returns" into `yield`, which transfers control to some other thread, as if the running thread had just voluntarily relinquished control.

Thread libraries typically disable signal delivery during scheduler calls.

## Multiprocessor Scheduling

Single-processor thread implementations can be extended to run on more than one OS process by arranging for processes to share the ready list and related data structures.

# Implementing Synchronization

Synchronization is the **primary challenge** for shared-memory concurrency.

We must prevent operations on different threads from interfering with each other and either...
* make some operations atomic
* delay the operation until appropriate preconditions hold.

Most often achived using **mutual exclusion locks**, which ensure that only one thread is only one thread is in a critical section at some point in time.

**Condition synchronization** lets a thread wait on some condition.
* It's important to note that this doesn't provide the *consensus* between threads required by mutual exclusion locks.

The implementation of parallel threads we've seen so far depends on atomicity and condition synchronization.
* Atomicity of operations on the ready list and related data structures ensures that they always satisfy a set of invariants.

> [!NOTE] Note
> 
> In general, we don't want to over-synchronize.

# Strategies

* **Busy-wait** synchronization
* **Spin Locks** provide mutual exclusion and *barriers*, which ensure that no thread passes beyond a given point in the program until *all* threads have reached that point.
* **Test and Set** uses logic that looks something like this:
  ```python
def test_and_set(flag: bool):
	old_value: bool = flag
	flag = True
	return (old_value == False)
```

This is an atomic operation, appearing to happen all at once.

Contention-reduction strategies:
* `test_and_test_and_set` lock, which spins with ordinary reads until the lock is free.
* Compare-and-swap
	* takes a location (or flag), the value we think it has now, and the new value.
	* Returns `true` or `false` to indicate whether or not a change was made.
	* If it has the value we expected, set it and return true to indicate a change was made. Otherwise, return false to indic.
* Reader-writer lock
	* Several threads are allowed to *read* the same location at the same time.
	* If any thread wants to write data, it needs to wait until *all current readers* finish.
	* All new readers must wait until writing is complete, and only one thread can write at a time.

