---
title: "2022-11-09"
date: 2022-11-09
---

# Review for Midterm 2

[[CS441 - Programming Languages/Midterm 2 Review|Main midterm review page here]]

## Garbage Collection

- Lock and Key
	- Each *heap object* gets a numeric key that is stored with the object
	- Each *pointer* becomes a `(key, address)` pair.
	- When the object is deallocated, the object's key is set to a specific "nope" value (so pointers will no longer match)
- Tombstone
	- Rather than pointing directly to an object, pointers will point to an intermediate "tombstone"
	- Problems
		- Extra step of indirection on *every single pointer access*
		- As the program continues to run, tombstones keep piling up
		- Whether a tombstone will ever be used again is generally undecidable, tricky even in small/specific cases
- Mark and Sweep
	- Initially, mark *everything* as garbage
	- Sweep through our program, mark everything that we can still reach as *not* garbage
	- Add garbage to *free list* - when new objects are allocated, you can use slots in memory
	- Problem: more of a "stop-the-world" approach. We've gotten better about this, it's not as disruptive as it used to be, but still.
- Reference Counting
	- Allocate a small amount of extra data
	- Problem
		- Reference counting will fail to catch cyclic references (tracing collectors will)
		  ![[images/ref-counting.svg]]
		- Doesn't deal with external fragmentation
- Stop and Copy
	- Similar to Mark and Sweep
		- Start the mark, go through the sweep, but then copy all the used data down into a particular *used* half of the heap. This solves the fragmentation problem.
- Generational
- Conservative
	- Instead of going through and looking through pointers, use some low-level asm instructions that let you blast through the stack really fast
		- Look for a bit pattern that looks like it *might* be a pointer to something on the heap
		- If it *looks* like a pointer, be safe and mark that block as being *in-use*
	- Faster, but may leave some garbage uncollected

With garbage collection, memory is typically either very ephemeral or very long-lived. The best predictor for how long it'll be needed is how long it's already been in existence.

## Passing Semantics

* Pass by value: value goes in and *cannot* be changed
* Pass by value-result: value goes in, then *whatever the value is at the end of the function gets copied back*.
* Pass by reference: 
* Pass by sharing
* Parameters can be in, out, or in-out
	* In: has to have a value assigned to it, function can read that value but not assign anything *to* it
	* Out: it doesn't have to be defined when you call it, the function can only write to it. Can always assign a new value, but can't see what value it has now.
	* In-out: more like standard call-by-reference

## Array Calculation

![[images/array-address-calculation.svg]]

Number of elements = $U - L + i$

$$sizeof(element) \cdot (U_i - L_i + i) \cdot (U_H - L_H + i) + (U_i - L_i) \cdot size$$

Benefits of a highly-orthogonal type system: no real need for coercion. If you have better rules about how things interact, you can check more things at compile-time.

```dialogue
< What are uinversal reference types?
> Void pointers!
< What are the advantages?
> They can be anything!
< What are the disadvantages?
> They can be anything!
```

Speaking of universal reference types, they're a big red flag that a type system is *not* orthogonal.

Stack smashing attack: stack forced to overflow

Dope vector: pass an array size as a function parameter
* problem: dont' know at compile time how much that is
* keep usual housekeeping stuff on the stack
* allocate *dope vector* - don't know how big the array is gonna be, but I *do* know how much space I need to store metadata (upper bound, lower bound, element size, etc.)
	* Remember: addressing everything as an offset from the frame pointer