---
title: "2022-11-30"
date: 2022-11-30
tags:
- lecture
- cs441
---

* Test-and-set locks to reduce contention
* Several processors (x86, IA-64, SPARC, etc) provide `compare_and_swap`
	* `CAS(location, expected, new)`
	* Using instructions like `atomic_add` or `compare_and_swap`, you can build ==fair== spin locks.
		* "fair" in the sense that threads are guaranteed to acquire the lock in the order they first ==attempt== to do so
	* Important variant: **reader-writer lock**
* Barriers
	* Barriers provide a mechanism to guarantee ==every== thread will complete the previous step before ==any== thread moves onto the next.
	* This is useful for data-parallel algorithms
	* For example, finite element analysis models a physical object as an enormous collection of tiny fragments.
	* Needs atomic fetch and decrement
	* Take a look at sample code in text
* Contention
	* sense-reversing barriers can lead to unacceptable levels of contention of large machines
* Nonblocking algorithms
	* we can avoid using a lock by using `compare_and_swap`
	  ```go
	  r1 := x
	  r2 := foo(x)
	  r2 := CAS(x, r1, r2) // replace x if it hasn't changed
	  if not r2 goto start
	  ```
	* This means CAS is a universal prpimitive for single-location atomic updates
* "Blocking"
	* A thread is "blocked" if it cannot make forward progress without action by other threads
	* An operation is *nonblocking* if, in every reachable state, any thread executing that operation is guaranteed to finish in a finite number of steps if it gets to run by itself with no further interference by other threads
	* This definition means locks are inherently blocking regardless of implementation!
		* if one thread holds a lock, no other thread that needs that lock can proceed
* Advantages of nonblocking algorithms
	* inherently tolerant of page faults and preemption
	* nonblocking algorithms are safe to use in signal (event) and interrupt handlers
	* can be faster than locks for things like stacks, queues, sorted lists, priority queues, hash tables, and memory management
	* Primarily used in library code because they can be subtle and difficult to devise
* Memory Consistency
	* we also have to worry about when writes become *visible* to different cores
	* full sequential consistency is extremely difficult to implement efficiently, so this isn't common
	* instead there's typically a relaxed memory model in which certain `load`s and `store`s might occur "out of order"
* The cost of ordering
	* For programs on a single core, every manufacturer guarantees that instructions will appear to occur in ==program order==

> [!WARNING]
> 
> Revisit this section (ordering) in the slides/book (p. 661). There's a lot here that I didn't take down.
* Scheduler Implementation
	* OS level processes must synchronize access to the ==ready list== and ==condition queues==, generally by spinning
	* Signals are disabled before entering scheduler code to protect the ready list and condition queues from concurrent access by a process and its own signal handler
		* we assume a single low-level lock that protects the entire scheduler
	* before saving its context block on the queue (yield or sleep_on), a thread must acquire the lock and, after returning from `reschedule`, release it
	* **Spin-then-Yield Lock** (fig. 13.14)
* Scheduler-based synchronization
	* busy-wait synchronization doesn't always make sense, most languages use scheduler-based synchronization
	* most common form of this is ==semaphores==
		* basically a counter with two associated operations, $P$ and $V$
			* (will continue)