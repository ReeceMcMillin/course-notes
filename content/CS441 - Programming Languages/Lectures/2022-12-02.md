---
title: "2022-12-02"
date: 2022-12-02
tags:
- lecture
- cs441
---

# Semaphores

* [[CS441 - Programming Languages/13 - Concurrency/Semaphores|Semaphores]] are basically counters with two operations, $P$ and $V$.
	* A thread that calls $V$ automatically increments the counter.
	* A thread that calls $P$ waits until the counter is positive, then decrements it.
* Generally require semaphores be ==fair==: threads complete $P$ operations in the order they started them.
* Problem: too low-level to enable well-structured, maintainable code
* Dijkstra's proposed solution: **monitors**

# Monitors

* A [[CS441 - Programming Languages/13 - Concurrency/Monitors|monitor]] is a module/object with its own operations, internal state, and a number of condition variables.
	* Only one operation allowed to be active at a given point in time
	* A thread that calls a busy monitor is automatically delayed until the monitor is free
	* Any operation may suspend itself by waiting on a condition variable, causing one of the waiting threads to be resumed
		* typically whichever had been waiting longest
* Monitors aren't *quite* the same as semaphores!
	* No memory

# Semantics

# Signals as Hints & Absolutes

* In general, you signal a thread when a condition it's waiting on becomes true
* In practice, context switching at the moment a signal is received is expensive to the scheduler
	* An alternative approach is to consider signals as hints
	* Move waiting thread to ready list, but let signaler retain control of the monitor
	* Waiting thread must recheck condition when it awakes
* Pros
	* Simpler implementation
* Cons
	* Less efficient
	* Doesn't necessarily guarantee FIFO scheduling
* Overall: trades ==potentially high== overhead in the ==worst== case for ==potentially low== overhead in the ==common== case.

# Nested Monitor Calls

* A  `wait` in a nested sequence of monitor operations will release mutual exclusion on the innermost monitor but leave the outer monitors locked
	* This can lead to deadlock if the only way out for another thread to reach a corresponding signal operation is through the same outer monitor
* Alternative: release exclusion on outer monitors while waiting in an inner one
	* adopted by several implementations for uniprocessors
	* drawback: requires monitor invariant hold not only at monitor exit and signal operations, but also ==any subroutine call that may result in a wait/signal in a nested monitor==.

# Multiple Conditions

# Lock Variables

# Transactional Memory

* Everything so far has just been a variant on a lock
	* tradeoff: easy to write data-race-free programs with single locks, but lock becomes a bottleneck that prevents the program from scaling effectively


> [!NOTE] Takeaway
> 
> Lock-based programs don't ==compose==!

We want something composable that just *snaps together* like legos

# Atomicity Without Locks

* Usual implementation is speculative:
	* Transactions on different threads proceed concurrently unless and until they conflict for access to some common record
	* In the absence of conflicts, they run perfectly in parallel
	* When conflicts arise, the underlying system arbitrates between the conflicting threads
		* One gets to continue and hopefully commit its updates
		* The other(s) abort, roll back their changes, and start over
* Allows significant parallelism to appear serial in some global total order at the level of program semantics

# Implicit Synchronization

# Futures
 
